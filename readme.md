## Setup

### Install Ollama for model inference
```
curl -fsSL https://ollama.com/install.sh | sh
```

### Run the command to install the model
```commandline
ollama pull phi3:mini
```

### Start Ollama server
```commandline
ollama serve
```

### Run the command to start the app:
```commandline
python main.py
```
